{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "061b80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0bad69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=[656,208]\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "80c6d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors=torch.tensor([[0.026687,  0.048969 ],\n",
    " [0.106309,  0.146625 ],\n",
    " [0.194006 , 0.330219 ],\n",
    " [0.057043  ,0.0817975],\n",
    " [0.009562,  0.243062 ],\n",
    " [0.041746 , 0.255281 ]]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f2c66728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f23c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ee43adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "darknet53 = timm.create_model('darknet53', pretrained=False, features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b40b5ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13133/170986702.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  darknet53.load_state_dict(torch.load(\"darknet53_feat.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darknet53.load_state_dict(torch.load(\"darknet53_feat.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e39fffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO(nn.Module):\n",
    "    def __init__(self,num_classes=9,anchor=6,freeze_backbone=True):\n",
    "        super(YOLO, self).__init__()\n",
    "        self.prediction=anchor*(num_classes+5)\n",
    "        self.backbone=darknet53    \n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.ml_pred=nn.Conv2d(256, self.prediction, kernel_size=1, stride=1)\n",
    "        self.dark_conv=nn.Conv2d(512, 256, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out=self.backbone.forward(x)\n",
    "        out=(self.dark_conv(out[4]))\n",
    "        return torch.sigmoid(self.ml_pred(out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f012529b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 84, 41, 13])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(1, 3, 656, 208)\n",
    "model=YOLO(num_classes=9)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d47eeae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_iou(anchors,targets,img_size=[656,208]):\n",
    "    W, H = img_size\n",
    "\n",
    "    # Convert normalized sizes to pixel space\n",
    "    target_w = (targets[0] * W)\n",
    "    target_h = (targets[1] * H)\n",
    "\n",
    "    anchor_w = (anchors[:, 0] * W).unsqueeze(1)\n",
    "    anchor_h = (anchors[:, 1] * H).unsqueeze(1)\n",
    "    \n",
    "    # print(target_w.shape,anchor_w.shape)\n",
    "    inter_w = torch.min(target_w, anchor_w)\n",
    "    inter_h = torch.min(target_h, anchor_h)\n",
    "   \n",
    "    inter_area = inter_w * inter_h\n",
    "    \n",
    "    union_area = (target_w * target_h) + (anchor_w * anchor_h)- inter_area        \n",
    "\n",
    "    iou = inter_area / (union_area + 1e-6)\n",
    "    best_anchor = torch.argmax(iou, dim=0)\n",
    "\n",
    "    return best_anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5080679c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta=torch.tensor([5,10])\n",
    "ta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b1ee910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labels(target,batch_size,num_prediction=9):\n",
    "    target = torch.tensor(target.to(device), dtype=torch.float32)  # shape: (N, 6)\n",
    "    b, cls, x1, y1, x2, y2 = target.T\n",
    "\n",
    "    # Initialize once\n",
    "    batch_target = torch.zeros((batch_size, 6,14, 41,13), dtype=torch.float32)\n",
    "\n",
    "    # Vectorize computation of coordinates\n",
    "    W, H = img_size\n",
    "    cell_w,cell_h=[656/41,208/13]\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    grid_x, grid_y = (cx / cell_w), (cy / cell_h)\n",
    "    tx, ty = (cx / cell_w)-grid_x, ((cy / cell_h))-grid_y\n",
    "    w, h = (x2 - x1) / W, (y2 - y1) / H\n",
    "\n",
    "    best_anchor = anchor_iou(anchors, torch.tensor([[w,h]]).to(device))\n",
    "    \n",
    "    # Assign everything in a batched manner\n",
    "    # # print(b)\n",
    "\n",
    "    \n",
    "    tw = torch.log((w / (anchors[best_anchor].permute(1,0)[0])) + 1e-6)\n",
    "    th = torch.log((h / (anchors[best_anchor].permute(1,0)[1])) + 1e-6)\n",
    "\n",
    "    batch_target[b-1, best_anchor, 0, grid_x, grid_y] = 1\n",
    "    batch_target[b-1, best_anchor,1, grid_x, grid_y] = tx\n",
    "    batch_target[b-1, best_anchor,2, grid_x, grid_y] = ty\n",
    "    batch_target[b-1, best_anchor,3, grid_x, grid_y] = tw\n",
    "    batch_target[b-1, best_anchor,4, grid_x, grid_y] = th\n",
    "\n",
    "    # One-hot for class predictions\n",
    "    for i in range(len(cls)):\n",
    "        batch_target[int(b[i]),best_anchor, 5 + int(cls[i]), grid_x[i], grid_y[i]] = 1\n",
    "    # print(batch_target.shape)\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8bb769ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2,obj_idx,anchors, eps=1e-7):\n",
    "    \n",
    "    tpx, tpy, tpw, tph = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "    tx, ty, tw, th = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "    W,H=img_size\n",
    "    pw = anchors[obj_idx[:, 1],0]  # assuming obj_idx[:,2] gives anchor index\n",
    "    ph = anchors[obj_idx[:, 1],1]\n",
    "    w = pw * torch.exp(tpw)\n",
    "    h = ph * torch.exp(tph)\n",
    "    w*=W\n",
    "    h*=H\n",
    "\n",
    "    cpx=torch.sigmoid(tpx)*16 + obj_idx[:,2]*16\n",
    "    cpy=torch.sigmoid(tpy)*16 + obj_idx[:,3]*16\n",
    "    \n",
    "\n",
    "    target_w = pw * torch.exp(tw)*W\n",
    "    target_h = ph * torch.exp(th)*H\n",
    "    target_cx = (tx + obj_idx[:,2]) * 16\n",
    "    target_cy = (ty + obj_idx[:,3]) * 16\n",
    "\n",
    "    box1 =  [cpx-w/2, cpy-h/2, cpx+w/2, cpy+h/2]\n",
    "    box2 =  [target_cx-target_w/2, target_cy-target_h/2, target_cx+target_w/2, target_cy+target_h/2]\n",
    "\n",
    "    xA = torch.max(box1[0], box2[0])\n",
    "    yA = torch.max(box1[1], box2[1])\n",
    "\n",
    "    xB = torch.min(box1[2], box2[2])\n",
    "    yB = torch.min(box1[3], box2[3])\n",
    "    \n",
    "    # union\n",
    "    area_intersection = torch.max(xB-xA, torch.zeros(xB.shape, dtype=xB.dtype, device=xB.device)) * torch.max(yB-yA, torch.zeros(yB.shape, dtype=yB.dtype, device=yB.device))\n",
    "\n",
    "    area_union = (box1[2]-box1[0]) * (box1[3]-box1[1]) + (box2[2]-box2[0]) * (box2[3]-box2[1]) - area_intersection +eps\n",
    "\n",
    "    iou = torch.clamp(area_intersection / (area_union ), 0, 1)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a5a16f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_loss(output, target,anchors, num_prediction=9):\n",
    "    batch_size=output.shape[0]\n",
    "    new_target=torch.tensor(process_labels(target,batch_size)).to(device) #-->(B,6,5+num_pred,41,13)\n",
    "    # new_target=torch.randn([1,6,14,41,13])\n",
    "    print(new_target.shape)\n",
    "    object_idx=new_target[:,:,0,:,:].nonzero()\n",
    "    print(object_idx.shape)\n",
    "    no_object_idx=(new_target[:,:,0,:,:]==0).nonzero()\n",
    "    new_output=output.view(batch_size,6,14,41,13)\n",
    "    \n",
    "    \n",
    "\n",
    "    target_obj=new_target[object_idx[:,0],object_idx[:,1],:,object_idx[:,2],object_idx[:,3]]\n",
    "    predicted_object = new_output[object_idx[:,0],object_idx[:,1],:,object_idx[:,2],object_idx[:,3]]\n",
    "    \n",
    "    print(predicted_object.shape)\n",
    "    iou=bbox_iou(predicted_object[:,1:5],target_obj[:,1:5],object_idx,anchors)\n",
    "\n",
    "    predicted_no_object = new_output[no_object_idx[:,0],no_object_idx[:,1],:,no_object_idx[:,2],no_object_idx[:,3]]\n",
    "\n",
    "    localization_loss = torch.sum((target_obj[:,1]-predicted_object[:,1])**2 +\n",
    "                                  (target_obj[:,2]-predicted_object[:,2])**2)\n",
    "\n",
    "    bbox_loss         = torch.sum((torch.sqrt(target_obj[:,3])-torch.sqrt(predicted_object[:,3]))**2 +\n",
    "                                  (torch.sqrt(target_obj[:,4])-torch.sqrt(predicted_object[:,4]))**2)\n",
    "    \n",
    "    pc_loss = torch.sum((target_obj[:,5:]-predicted_object[:,5:])**2)\n",
    "    \n",
    "    ### IOU \n",
    "    iou=bbox_iou(predicted_object[:,1:5],target_obj[:,1:5],object_idx)\n",
    "\n",
    "    obj_conf_loss=torch.sum((iou.detach()-predicted_object[:,0])**2)\n",
    "    nobj_confidence_loss= torch.sum((0 - predicted_no_object[:, 0])**2)\n",
    "\n",
    "    loss = 5.0*(localization_loss + bbox_loss) + obj_conf_loss + 0.5*nobj_confidence_loss + pc_loss\n",
    "    # print(localization_loss,obj_conf_loss)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4abb529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = []\n",
    "    targets = []\n",
    "    for i, (img, labels) in enumerate(batch):\n",
    "        imgs.append(img)\n",
    "\n",
    "        if labels.numel() > 0:\n",
    "            # add batch index column\n",
    "            batch_idx = torch.full((labels.shape[0], 1), i) \n",
    "            labels = torch.cat((batch_idx, labels), dim=1)\n",
    "            targets.append(labels)\n",
    "\n",
    "    imgs = torch.stack(imgs)\n",
    "    if len(targets):\n",
    "        targets = torch.cat(targets, dim=0)\n",
    "    else:\n",
    "        targets = torch.zeros((0, 6))\n",
    "    return imgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "08bb4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, img_size=(640,192), transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith(\".jpg\")]\n",
    "        self.img_size = img_size\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, img_name.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "        # --- Load and normalize image ---\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)  # shape: [3, H, W], range [0,1]\n",
    "        image=image.permute(0,2,1)\n",
    "        # --- Load YOLO labels ---\n",
    "        boxes = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    cls, x1, y1, x2, y2 = map(float, parts)\n",
    "                    boxes.append([cls, x1, y1, x2, y2])\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "        else:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        return image, boxes\n",
    "    \n",
    "dataset = YOLODataset(\"../data/656x208/images\", \"../data/656x208/label\")\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "755c7366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13133/2454238090.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target.to(device), dtype=torch.float32)  # shape: (N, 6)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[160]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     41\u001b[39m     out = model(imgs)\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# print(imgs.shape)\u001b[39;00m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# print(targets.shape) #([N, 6]) -> (B, cls,cx,cy,w,h)\u001b[39;00m\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# print(out.shape)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m loss=\u001b[43myolo_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# # -----------------------------\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# # Backward + Step\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# # -----------------------------\u001b[39;00m\n\u001b[32m     50\u001b[39m scaler.scale(loss).backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[157]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36myolo_loss\u001b[39m\u001b[34m(output, target, anchors, num_prediction)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34myolo_loss\u001b[39m(output, target,anchors, num_prediction=\u001b[32m9\u001b[39m):\n\u001b[32m      2\u001b[39m     batch_size=output.shape[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     new_target=torch.tensor(\u001b[43mprocess_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m).to(device) \u001b[38;5;66;03m#-->(B,6,5+num_pred,41,13)\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# new_target=torch.randn([1,6,14,41,13])\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(new_target.shape)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[155]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mprocess_labels\u001b[39m\u001b[34m(target, batch_size, num_prediction)\u001b[39m\n\u001b[32m     13\u001b[39m tx, ty = (cx / cell_w)-grid_x, ((cy / cell_h))-grid_y\n\u001b[32m     14\u001b[39m w, h = (x2 - x1) / W, (y2 - y1) / H\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m best_anchor = anchor_iou(anchors, \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43mh\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.to(device))\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Assign everything in a batched manner\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# # print(b)\u001b[39;00m\n\u001b[32m     22\u001b[39m tw = torch.log((w / (anchors[best_anchor].permute(\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m)[\u001b[32m0\u001b[39m])) + \u001b[32m1e-6\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# =========================\n",
    "# TRAINING CONFIG\n",
    "# =========================\n",
    "num_epochs = 10\n",
    "\n",
    "# --- Model / Loss / Optimizer ---\n",
    "model = YOLO(num_classes=9).to('cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "\n",
    "log_interval = 100  # print every N batches\n",
    "scaler = GradScaler(device)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "print(f\"Starting training on {device}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_idx, (imgs, targets) in enumerate(loader):\n",
    "        # -----------------------------\n",
    "        # Move to GPU\n",
    "        # -----------------------------\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)  # [batch_id, cls, x, y, w, h]\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Forward + Loss (with AMP)\n",
    "        # -----------------------------\n",
    "        with autocast(\"cuda\"):\n",
    "            out = model(imgs)\n",
    "            # print(imgs.shape)\n",
    "            # print(targets.shape) #([N, 6]) -> (B, cls,cx,cy,w,h)\n",
    "            # print(out.shape)\n",
    "            \n",
    "        loss=yolo_loss(out,targets,anchors)\n",
    "        # # -----------------------------\n",
    "        # # Backward + Step\n",
    "        # # -----------------------------\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # # -----------------------------\n",
    "        # # Logging\n",
    "        # # -----------------------------\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "                f\"[Batch {batch_idx}/{len(loader)}] \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "        # # cleanup\n",
    "        # del imgs, targets, out\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Epoch summary\n",
    "    # -----------------------------\n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# PLOT LOSS CURVE\n",
    "# =========================\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', linewidth=2)\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from PIL import Image, ImageDraw, ImageFont\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import random\n",
    "\n",
    "# # ======= CONFIG =======\n",
    "# IMG_PATH = \"./data/640x192/images\"   # change this\n",
    "\n",
    "# NUM_CLASSES = 9\n",
    "# S = 20  # your grid size\n",
    "\n",
    "# IMG_SIZE = (640, 192)\n",
    "\n",
    "# # ======= LOAD MODEL =======\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# # ======= PREPROCESS IMAGE =======\n",
    "\n",
    "\n",
    "# IMG_DIR = \"../data/640x192/images\"\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # pick a random image file\n",
    "# img_name = random.choice([f for f in os.listdir(IMG_DIR) if f.endswith((\".jpg\", \".png\"))])\n",
    "# # img_name='000002.jpg'\n",
    "# img_path = os.path.join(IMG_DIR, img_name)\n",
    "\n",
    "# # open and preprocess image\n",
    "# img = Image.open(img_path).convert(\"RGB\")\n",
    "# img_tensor = (\n",
    "#     torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0\n",
    "# ).unsqueeze(0).to(device) \n",
    "\n",
    "# img_tensor=img_tensor.permute(0,1,3,2).to('cpu')\n",
    "# # ======= INFERENCE =======\n",
    "# with torch.no_grad():\n",
    "#     preds = model(img_tensor)[0]  # (C, S, S)\n",
    "\n",
    "# conf = preds[0]        # confidence map\n",
    "# x = preds[1]\n",
    "# y = preds[2]\n",
    "# w = preds[3]\n",
    "# h = preds[4]\n",
    "# cls_scores = torch.softmax(preds[5:], dim=0)\n",
    "\n",
    "# # ======= DECODE BOXES =======\n",
    "# boxes = []\n",
    "# for i in range(40):\n",
    "#     for j in range(12):\n",
    "#         if conf[i, j] > 0.02:\n",
    "#             cx = (i + x[i, j]) * (640/40)\n",
    "#             cy = (j + y[i, j]) * (192/12)\n",
    "#             bw = w[i, j] * IMG_SIZE[0]\n",
    "#             bh = h[i, j] * IMG_SIZE[1]\n",
    "\n",
    "#             x1 = cx - bw / 2\n",
    "#             y1 = cy - bh / 2\n",
    "#             x2 = cx + bw / 2\n",
    "#             y2 = cy + bh / 2\n",
    "\n",
    "#             cls_idx = torch.argmax(cls_scores[:, i, j]).item()\n",
    "#             boxes.append((x1, y1, x2, y2, cls_idx, conf[i, j].item()))\n",
    "\n",
    "# # # ======= VISUALIZE =======\n",
    "# # draw = ImageDraw.Draw(img)\n",
    "# # for (x1, y1, x2, y2, cls, conf) in boxes:\n",
    "# #     draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "# #     draw.text((x1, y1 - 10), f\"{cls}:{conf:.6f}\", fill=\"yellow\")\n",
    "\n",
    "# plt.figure(figsize=(20, 6))\n",
    "# plt.imshow(img)\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# print(preds.shape)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.imshow(preds[0,:].permute(1,0).cpu(), cmap='inferno', interpolation='nearest')\n",
    "# plt.colorbar(label='Confidence')\n",
    "# plt.title(\"Confidence Map (40Ã—12 grid)\")\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
